{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a6ae96",
   "metadata": {},
   "source": [
    "-- Checkpoint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb29b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. See more details on this model's documentation page: `https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "Input: saya belajar\n",
      "Phoneme: sˈaja bəlˈadʒar\n",
      "\n",
      "Input: learning is fun\n",
      "With masking: ləɣarɣiŋɣˈis fˈun\n",
      "Without masking: ləˈarniŋ ˈis fˈun\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import pickle\n",
    "from transformers import AlbertConfig, AlbertModel, TransfoXLTokenizer\n",
    "from model import MultiTaskModel\n",
    "from utils import length_to_mask\n",
    "from text_utils import TextCleaner\n",
    "from phonemize import phonemize\n",
    "\n",
    "# Load config\n",
    "config_path = \"Configs/config.yml\"\n",
    "config = yaml.safe_load(open(config_path))\n",
    "\n",
    "# Load token_maps\n",
    "with open(config['dataset_params']['token_maps'], 'rb') as handle:\n",
    "    token_maps = pickle.load(handle)\n",
    "\n",
    "# Load tokenizer\n",
    "os.environ['TRUST_REMOTE_CODE'] = 'True'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(config['dataset_params']['tokenizer'])\n",
    "\n",
    "# Setup phonemizer\n",
    "from phonemizer import phonemize as phonemizer_phonemize\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "global_phonemizer = phonemizer_phonemize.backend = EspeakBackend(language='id', preserve_punctuation=True, with_stress=True)\n",
    "\n",
    "# Load model\n",
    "albert_base_configuration = AlbertConfig(**config['model_params'])\n",
    "bert = AlbertModel(albert_base_configuration)\n",
    "bert = MultiTaskModel(bert, \n",
    "                      num_vocab=1 + max([m['token'] for m in token_maps.values()]), \n",
    "                      num_tokens=config['model_params']['vocab_size'],\n",
    "                      hidden_size=config['model_params']['hidden_size'])\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"/workspace/src/PL-BERT-ID/step_1000000.t7\"  # Ganti dengan path checkpoint Anda\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "state_dict = checkpoint['net']\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith('module.') else k\n",
    "    new_state_dict[name] = v\n",
    "bert.load_state_dict(new_state_dict, strict=False)\n",
    "bert.eval()\n",
    "\n",
    "# TextCleaner for phoneme to IDs\n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "# Inverse dict for decoding phonemes\n",
    "from text_utils import symbols\n",
    "phoneme_dict = {i: symbols[i] for i in range(len(symbols))}\n",
    "\n",
    "# Function untuk inference text ke phoneme (langsung)\n",
    "def text_to_phoneme(text):\n",
    "    \"\"\"Konversi text ke phoneme menggunakan phonemizer\"\"\"\n",
    "    phoneme_data = phonemize(text, global_phonemizer, tokenizer)\n",
    "    phonemes = phoneme_data['phonemes']\n",
    "    \n",
    "    # Gabungkan phoneme dengan spasi\n",
    "    phoneme_str = ' '.join(phonemes)\n",
    "    \n",
    "    return phoneme_str\n",
    "\n",
    "# Function untuk phoneme prediction dengan masking\n",
    "def infer_with_masking(text, mask_positions=None):\n",
    "    \"\"\"Prediksi phoneme dengan beberapa posisi di-mask\"\"\"\n",
    "    # Phonemize input text\n",
    "    phoneme_data = phonemize(text, global_phonemizer, tokenizer)\n",
    "    phonemes = phoneme_data['phonemes']\n",
    "    \n",
    "    # Build phoneme string dengan separator\n",
    "    phoneme_parts = []\n",
    "    for ph in phonemes:\n",
    "        phoneme_parts.append(ph)\n",
    "        phoneme_parts.append(' ')  # token_separator\n",
    "    phoneme_str = ''.join(phoneme_parts).strip()\n",
    "    \n",
    "    # Convert ke IDs\n",
    "    phoneme_ids = text_cleaner(phoneme_str)\n",
    "    \n",
    "    # Apply masking jika ada\n",
    "    if mask_positions:\n",
    "        mask_token_id = text_cleaner(\"M\")[0]  # M adalah mask token\n",
    "        for pos in mask_positions:\n",
    "            if pos < len(phoneme_ids):\n",
    "                phoneme_ids[pos] = mask_token_id\n",
    "    \n",
    "    input_ids = torch.tensor([phoneme_ids]).long()\n",
    "    input_lengths = [len(phoneme_ids)]\n",
    "    \n",
    "    # Create mask\n",
    "    text_mask = length_to_mask(torch.tensor(input_lengths)).bool()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokens_pred, words_pred = bert(input_ids, attention_mask=(~text_mask).int())\n",
    "    \n",
    "    # Decode hanya posisi yang di-mask\n",
    "    if mask_positions:\n",
    "        predicted_phoneme_ids = torch.argmax(tokens_pred[0], dim=-1).cpu().numpy()\n",
    "        \n",
    "        result_ids = phoneme_ids.copy()\n",
    "        for pos in mask_positions:\n",
    "            if pos < len(predicted_phoneme_ids):\n",
    "                result_ids[pos] = predicted_phoneme_ids[pos]\n",
    "        \n",
    "        decoded_phonemes = ''.join(phoneme_dict.get(pid, '?') for pid in result_ids)\n",
    "        return decoded_phonemes\n",
    "    else:\n",
    "        # Tanpa masking, return input phoneme\n",
    "        return phoneme_str\n",
    "\n",
    "# Contoh penggunaan 1: Text to Phoneme langsung\n",
    "input_text = \"saya belajar\"\n",
    "phoneme_output = text_to_phoneme(input_text)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Phoneme: {phoneme_output}\")\n",
    "\n",
    "# Contoh penggunaan 2: Inference dengan masking\n",
    "input_text2 = \"learning is fun\"\n",
    "# Mask beberapa posisi untuk testing\n",
    "masked_output = infer_with_masking(input_text2, mask_positions=[2, 5, 8])\n",
    "print(f\"\\nInput: {input_text2}\")\n",
    "print(f\"With masking: {masked_output}\")\n",
    "\n",
    "# Contoh tanpa masking\n",
    "normal_output = infer_with_masking(input_text2)\n",
    "print(f\"Without masking: {normal_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PL-BERT-ID",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
