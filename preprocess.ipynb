{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (Indonesia) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52b79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemize import phonemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44fe0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: /workspace/PL-BERT/preprocess.ipynb\n",
    "# Tambahkan kode phonemizer kustom Anda di sini\n",
    "import subprocess\n",
    "import re\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import warnings\n",
    "from tqdm import tqdm \n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Trying to detect language from a single word.\")\n",
    "\n",
    "languages = [Language.ENGLISH, Language.INDONESIAN]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def detect_lang(word: str) -> str:\n",
    "    result = detector.detect_language_of(word)\n",
    "    if result is None:\n",
    "        return \"id\"\n",
    "    return \"en\" if result == Language.ENGLISH else \"id\"\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def phonemize_word(word: str, ipa: bool, keep_stress: bool, sep: str) -> str:\n",
    "    lang = detect_lang(word)\n",
    "    lang_map = {\"id\": \"id\", \"en\": \"en-us\"}\n",
    "    voice = lang_map.get(lang, \"id\")\n",
    "    cmd = [\"espeak-ng\", \"-v\", voice, \"-q\", f\"--sep={sep}\", word]\n",
    "    if ipa:\n",
    "        cmd.insert(3, \"--ipa\")\n",
    "    else:\n",
    "        cmd.insert(3, \"-x\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True)\n",
    "        phonemes = result.stdout.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "        phonemes = phonemes.replace(\"\\ufeff\", \"\")\n",
    "        if not keep_stress:\n",
    "            phonemes = re.sub(r\"[ˈˌ]\", \"\", phonemes)\n",
    "        return phonemes\n",
    "    except (subprocess.TimeoutExpired, Exception):\n",
    "        return word\n",
    "\n",
    "class EnIndPhonemizer:\n",
    "    def __init__(self, ipa=True, keep_stress=False, sep=\"\", max_workers=None):\n",
    "        self.ipa = ipa\n",
    "        self.keep_stress = keep_stress\n",
    "        self.sep = sep\n",
    "        self.max_workers = max_workers or 4\n",
    "\n",
    "    def phonemize(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        words = text.strip().split()\n",
    "        phonemized_words = [\n",
    "            phonemize_word(w, self.ipa, self.keep_stress, self.sep) for w in words\n",
    "        ]\n",
    "        return \" \".join(phonemized_words)\n",
    "\n",
    "    def process_in_parallel(self, texts: list[str]) -> list[str]:\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            return list(\n",
    "                tqdm(\n",
    "                    executor.map(self.phonemize, texts),\n",
    "                    total=len(texts),\n",
    "                    desc=\"Phonemizing Sentences\"\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b363b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonemizer\n",
    "global_phonemizer = EnIndPhonemizer(ipa=True, keep_stress=True, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeefa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRUST_REMOTE_CODE'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/PL-BERT-ID/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. See more details on this model's documentation page: `https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TransfoXLTokenizer\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb456f6",
   "metadata": {},
   "source": [
    "Since using load_dataset with the Indonesian Wikipedia (id) resulted in errors (e.g., \"Not Found\"), we will download and load the dataset manually.\n",
    "\n",
    "You can download the dataset from this link: https://huggingface.co/datasets/wikimedia/wikipedia/tree/main/20231101.id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 665622\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Use a glob pattern to load all Parquet files in the 'wikipedia' folder.\n",
    "# This pattern will search for all files ending with '.parquet' within the folder.\n",
    "parquet_folder = \"/workspace/src/PL-BERT-ID/wikipedia/*.parquet\"\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"parquet\", data_files=parquet_folder)\n",
    "    if isinstance(dataset, dict) or hasattr(dataset, \"keys\"):\n",
    "        split_name = \"train\" if \"train\" in dataset else list(dataset.keys())[0]\n",
    "        dataset = dataset[split_name]\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(dataset)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./wiki_phoneme\" # set up root directory for multiprocessor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_shards =100\n",
    "\n",
    "def process_shard(i):\n",
    "    directory = root_directory + \"/shard_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "    processed_dataset = shard.map(lambda t: {\n",
    "        'phonemes': global_phonemizer.phonemize(t['text']),\n",
    "        'input_ids': tokenizer.encode(t['text'])  # Tambahkan tokenization jika diperlukan\n",
    "    }, remove_columns=['text'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73caf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You will need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you will need to change the timeout to a longer value to make more shards processed before being killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 9 ...Processing shard 3 ...Processing shard 1 ...Processing shard 0 ...Processing shard 2 ...\n",
      "\n",
      "Shard 11 already exists!Shard 8 already exists!\n",
      "Shard 4 already exists!\n",
      "Shard 6 already exists!Shard 7 already exists!Shard 10 already exists!Shard 5 already exists!\n",
      "Shard 14 already exists!Shard 13 already exists!Shard 12 already exists!\n",
      "\n",
      "Shard 15 already exists!\n",
      "Shard 16 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 17 already exists!\n",
      "\n",
      "\n",
      "Shard 18 already exists!Shard 20 already exists!Shard 21 already exists!\n",
      "Shard 22 already exists!Shard 23 already exists!Shard 19 already exists!\n",
      "Shard 24 already exists!\n",
      "Shard 25 already exists!\n",
      "Shard 26 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 29 already exists!Shard 28 already exists!Shard 27 already exists!Shard 30 already exists!Shard 31 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_shard.<locals>.<lambda> at 0x74bf3b28ccc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 32 already exists!\n",
      "\n",
      "Shard 34 already exists!Shard 33 already exists!\n",
      "\n",
      "\n",
      "\n",
      "Shard 35 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_shard.<locals>.<lambda> at 0x74bf3b28ccc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 37 already exists!Shard 36 already exists!\n",
      "Shard 40 already exists!Shard 41 already exists!Shard 38 already exists!Shard 39 already exists!Shard 42 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_shard.<locals>.<lambda> at 0x74bf3b28ccc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shard 43 already exists!\n",
      "\n",
      "\n",
      "Shard 44 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_shard.<locals>.<lambda> at 0x74bf3b28ccc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 46 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_shard.<locals>.<lambda> at 0x74bf3b28ccc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 45 already exists!Shard 47 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 48 already exists!\n",
      "\n",
      "\n",
      "\n",
      "Shard 49 already exists!Shard 51 already exists!Shard 50 already exists!Shard 52 already exists!Shard 55 already exists!Shard 53 already exists!\n",
      "\n",
      "Shard 54 already exists!\n",
      "Shard 56 already exists!\n",
      "\n",
      "\n",
      "\n",
      "Shard 58 already exists!Shard 57 already exists!Shard 59 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 60 already exists!Shard 61 already exists!Shard 62 already exists!Shard 64 already exists!Shard 63 already exists!Shard 65 already exists!Shard 66 already exists!Shard 71 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 70 already exists!Shard 67 already exists!Shard 69 already exists!Shard 68 already exists!Shard 73 already exists!Shard 75 already exists!Shard 74 already exists!Shard 72 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/6657 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 77 already exists!\n",
      "\n",
      "Shard 76 already exists!Shard 82 already exists!\n",
      "Shard 80 already exists!Shard 78 already exists!Shard 81 already exists!Shard 79 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/6657 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shard 83 already exists!Shard 84 already exists!\n",
      "\n",
      "Shard 90 already exists!Shard 87 already exists!Shard 85 already exists!Shard 89 already exists!Shard 86 already exists!Shard 88 already exists!Shard 91 already exists!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/6657 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 94 already exists!Shard 92 already exists!Shard 96 already exists!Shard 97 already exists!Shard 95 already exists!Shard 93 already exists!Shard 99 already exists!Shard 98 already exists!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/6657 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 6657/6657 [00:00<00:00, 38615.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6657/6657 [00:00<00:00, 38395.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6657/6657 [00:00<00:00, 38670.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6657/6657 [00:00<00:00, 38238.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6657/6657 [00:00<00:00, 38071.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "max_workers = 20\n",
    "failed_shards = []\n",
    "with ProcessPool(max_workers=max_workers) as pool:\n",
    "    future = pool.map(process_shard, range(num_shards), timeout=600)\n",
    "    for i, result in enumerate(future.result()):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"Shard {i} failed: {result}\")\n",
    "            failed_shards.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66742159",
   "metadata": {},
   "outputs": [],
   "source": [
    "if failed_shards:\n",
    "    print(\"Retrying failed shards...\")\n",
    "    with ProcessPool(max_workers=max_workers) as pool:\n",
    "        pool.map(process_shard, failed_shards, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0568da38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_0 loaded\n",
      "shard_1 loaded\n",
      "shard_10 loaded\n",
      "shard_11 loaded\n",
      "shard_12 loaded\n",
      "shard_13 loaded\n",
      "shard_14 loaded\n",
      "shard_15 loaded\n",
      "shard_16 loaded\n",
      "shard_17 loaded\n",
      "shard_18 loaded\n",
      "shard_19 loaded\n",
      "shard_2 loaded\n",
      "shard_20 loaded\n",
      "shard_21 loaded\n",
      "shard_22 loaded\n",
      "shard_23 loaded\n",
      "shard_24 loaded\n",
      "shard_25 loaded\n",
      "shard_26 loaded\n",
      "shard_27 loaded\n",
      "shard_28 loaded\n",
      "shard_29 loaded\n",
      "shard_3 loaded\n",
      "shard_30 loaded\n",
      "shard_31 loaded\n",
      "shard_32 loaded\n",
      "shard_33 loaded\n",
      "shard_34 loaded\n",
      "shard_35 loaded\n",
      "shard_36 loaded\n",
      "shard_37 loaded\n",
      "shard_38 loaded\n",
      "shard_39 loaded\n",
      "shard_4 loaded\n",
      "shard_40 loaded\n",
      "shard_41 loaded\n",
      "shard_42 loaded\n",
      "shard_43 loaded\n",
      "shard_44 loaded\n",
      "shard_45 loaded\n",
      "shard_46 loaded\n",
      "shard_47 loaded\n",
      "shard_48 loaded\n",
      "shard_49 loaded\n",
      "shard_5 loaded\n",
      "shard_50 loaded\n",
      "shard_51 loaded\n",
      "shard_52 loaded\n",
      "shard_53 loaded\n",
      "shard_54 loaded\n",
      "shard_55 loaded\n",
      "shard_56 loaded\n",
      "shard_57 loaded\n",
      "shard_58 loaded\n",
      "shard_59 loaded\n",
      "shard_6 loaded\n",
      "shard_60 loaded\n",
      "shard_61 loaded\n",
      "shard_62 loaded\n",
      "shard_63 loaded\n",
      "shard_64 loaded\n",
      "shard_65 loaded\n",
      "shard_66 loaded\n",
      "shard_67 loaded\n",
      "shard_68 loaded\n",
      "shard_69 loaded\n",
      "shard_7 loaded\n",
      "shard_70 loaded\n",
      "shard_71 loaded\n",
      "shard_72 loaded\n",
      "shard_73 loaded\n",
      "shard_74 loaded\n",
      "shard_75 loaded\n",
      "shard_76 loaded\n",
      "shard_77 loaded\n",
      "shard_78 loaded\n",
      "shard_79 loaded\n",
      "shard_8 loaded\n",
      "shard_80 loaded\n",
      "shard_81 loaded\n",
      "shard_82 loaded\n",
      "shard_83 loaded\n",
      "shard_84 loaded\n",
      "shard_85 loaded\n",
      "shard_86 loaded\n",
      "shard_87 loaded\n",
      "shard_88 loaded\n",
      "shard_89 loaded\n",
      "shard_9 loaded\n",
      "shard_90 loaded\n",
      "shard_91 loaded\n",
      "shard_92 loaded\n",
      "shard_93 loaded\n",
      "shard_94 loaded\n",
      "shard_95 loaded\n",
      "shard_96 loaded\n",
      "shard_97 loaded\n",
      "shard_98 loaded\n",
      "shard_99 loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "output = [dI for dI in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory,dI))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = root_directory + \"/\" + o\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(\"%s loaded\" % o)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 665637/665637 [00:11<00:00, 58972.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to wikipedia_20220301.en.processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'phonemes', 'input_ids'],\n",
       "    num_rows: 665637\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5200/5200 [00:38<00:00, 133.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1445662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172557/172557 [00:02<00:00, 69194.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# get each token's lower case\n",
    "\n",
    "lower_tokens = []\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    if word.lower() != word:\n",
    "        t = tokenizer.encode([word.lower()])[0]\n",
    "        lower_tokens.append(t)\n",
    "    else:\n",
    "        lower_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa2dea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = (list(set(lower_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172557/172557 [00:15<00:00, 11251.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# redo the mapping for lower number of tokens\n",
    "\n",
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    word = word.lower()\n",
    "    new_t = tokenizer.encode([word.lower()])[0]\n",
    "    token_maps[t] = {'word': word, 'token': lower_tokens.index(new_t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapper saved to token_maps.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70874215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Microsoft Edge']\n",
      "Bad pipe message: %s [b'v=\"140\"\\r\\nsec-ch-ua-mobile: ?0\\r\\nse', b'ch-ua-platform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0;', b'in64; x64) AppleWebKit/537.36 (', b'TML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0\\r\\nAccept: tex']\n",
      "Bad pipe message: %s [b'html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exch', b'ge;v=b3;q=0.7\\r\\nSec-Fetch-Site: none\\r\\nSec-Fetch-Mode: navigate\\r\\nSec-Fetch-User: ?1\\r\\nSec-Fetch-Des']\n",
      "Bad pipe message: %s [b'ol: max-age=0\\r\\nsec-ch-ua: \"Chromium\";v=\"140\", \"Not=A?Brand\";v=\"24\", \"Microsoft Edge\";v=\"140\"\\r\\nsec-ch-ua-mobile: ?0\\r']\n",
      "Bad pipe message: %s [b'ec-ch-ua-', b'atform: \"Windows\"\\r\\nUpgrade-Insecure-Requests: 1\\r\\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb']\n",
      "Bad pipe message: %s [b't/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0\\r', b'ccept: te', b'/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchang']\n"
     ]
    }
   ],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PL-BERT-ID",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
