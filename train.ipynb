{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc4ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/PL-BERT-ID/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import LoggerType\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import AlbertConfig, AlbertModel\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from model import MultiTaskModel\n",
    "from dataloader import build_dataloader\n",
    "from utils import length_to_mask, scan_checkpoint\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d0c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "\n",
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a7f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(config['dataset_params']['token_maps'], 'rb') as handle:\n",
    "    token_maps = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158bf338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`TransfoXL` was deprecated due to security issues linked to `pickle.load` in `TransfoXLTokenizer`. See more details on this model's documentation page: `https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/transfo-xl.md`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRUST_REMOTE_CODE'] = 'True'\n",
    "\n",
    "from transformers import TransfoXLTokenizer, TransfoXLModel\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(config['dataset_params']['tokenizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e60819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # F0 loss (regression)\n",
    "\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "log_interval = config['log_interval']\n",
    "save_interval = config['save_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ef5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/workspace/src/PL-BERT-ID/step_1000000.t7\"  \n",
    "fine_tune = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fa9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    \n",
    "    # Initialize accelerator first\n",
    "    accelerator = Accelerator(mixed_precision=config['mixed_precision'], split_batches=True, kwargs_handlers=[ddp_kwargs])\n",
    "    \n",
    "    curr_steps = 0\n",
    "    \n",
    "    dataset = load_from_disk(config[\"data_folder\"])\n",
    "\n",
    "    log_dir = config['log_dir']\n",
    "    if not osp.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n",
    "    shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "    \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = build_dataloader(dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    num_workers=0, \n",
    "                                    dataset_config=config['dataset_params'])\n",
    "\n",
    "    albert_base_configuration = AlbertConfig(**config['model_params'])\n",
    "    \n",
    "    bert = AlbertModel(albert_base_configuration)\n",
    "    bert = MultiTaskModel(bert, \n",
    "                          num_vocab=1 + max([m['token'] for m in token_maps.values()]), \n",
    "                          num_tokens=config['model_params']['vocab_size'],\n",
    "                          hidden_size=config['model_params']['hidden_size'])\n",
    "    \n",
    "    load = True\n",
    "    iters = 0  # Initialize iters\n",
    "    \n",
    "    if fine_tune and osp.exists(checkpoint_path):\n",
    "        # Fine-tune dari checkpoint spesifik\n",
    "        checkpoint_step = int(checkpoint_path.split('_')[-1].split('.')[0])  \n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = checkpoint['net']\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] if k.startswith('module.') else k  \n",
    "            new_state_dict[name] = v\n",
    "        bert.load_state_dict(new_state_dict, strict=False)\n",
    "        # Reset iters untuk fine-tuning, atau gunakan step dari checkpoint\n",
    "        iters = 0  # Mulai dari 0 untuk fine-tuning\n",
    "        # Atau jika ingin melanjutkan: iters = checkpoint_step\n",
    "        accelerator.print(f'Fine-tuning from checkpoint: {checkpoint_path}, starting from step {iters}')\n",
    "    else:\n",
    "        try:\n",
    "            files = os.listdir(log_dir)\n",
    "            ckpts = []\n",
    "            for f in os.listdir(log_dir):\n",
    "                if f.startswith(\"step_\"): ckpts.append(f)\n",
    "\n",
    "            checkpoint_iters = [int(f.split('_')[-1].split('.')[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\n",
    "            if checkpoint_iters:\n",
    "                iters = sorted(checkpoint_iters)[-1]\n",
    "            else:\n",
    "                iters = 0\n",
    "                load = False\n",
    "        except:\n",
    "            iters = 0\n",
    "            load = False\n",
    "    \n",
    "    optimizer = AdamW(bert.parameters(), lr=1e-4)\n",
    "    \n",
    "    if load and not fine_tune and iters > 0:\n",
    "        checkpoint_file = log_dir + \"/step_\" + str(iters) + \".t7\"\n",
    "        if osp.exists(checkpoint_file):\n",
    "            checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            accelerator.print(f'Loaded checkpoint from step {iters}')\n",
    "    \n",
    "    bert, optimizer, train_loader = accelerator.prepare(\n",
    "        bert, optimizer, train_loader\n",
    "    )\n",
    "\n",
    "    accelerator.print(f'Start training from step {iters}...')\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for _, batch in enumerate(train_loader):        \n",
    "        # Check if we've reached the maximum steps\n",
    "        if iters >= num_steps:\n",
    "            accelerator.print(f'Reached maximum steps ({num_steps}). Stopping training.')\n",
    "            return\n",
    "            \n",
    "        curr_steps += 1\n",
    "\n",
    "        batch = [b.to(accelerator.device) if hasattr(b, \"to\") else b for b in batch]\n",
    "        words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "\n",
    "        text_mask = length_to_mask(torch.Tensor(input_lengths)).to(accelerator.device)\n",
    "        tokens_pred, words_pred = bert(phonemes, attention_mask=(~text_mask).int())\n",
    "        \n",
    "        loss_vocab = 0\n",
    "        for _s2s_pred, _text_input, _text_length, _masked_indices in zip(words_pred, words, input_lengths, masked_indices):\n",
    "            loss_vocab += criterion(_s2s_pred[:_text_length], \n",
    "                                        _text_input[:_text_length])\n",
    "        loss_vocab /= words.size(0)\n",
    "        \n",
    "        loss_token = 0\n",
    "        sizes = 1\n",
    "        for _s2s_pred, _text_input, _text_length, _masked_indices in zip(tokens_pred, labels, input_lengths, masked_indices):\n",
    "            if len(_masked_indices) > 0:\n",
    "                _text_input = _text_input[:_text_length][_masked_indices]\n",
    "                loss_tmp = criterion(_s2s_pred[:_text_length][_masked_indices], \n",
    "                                            _text_input[:_text_length]) \n",
    "                loss_token += loss_tmp\n",
    "                sizes += 1\n",
    "        loss_token /= sizes\n",
    "\n",
    "        loss = loss_vocab + loss_token\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        iters += 1  # Increment setelah training step\n",
    "        \n",
    "        if iters % log_interval == 0:\n",
    "            # Fix the warning by detaching tensors\n",
    "            accelerator.print ('Step [%d/%d], Loss: %.5f, Vocab Loss: %.5f, Token Loss: %.5f'\n",
    "                    %(iters, num_steps, running_loss / log_interval, loss_vocab.detach().item(), loss_token.detach().item()))\n",
    "            running_loss = 0\n",
    "            \n",
    "        if iters % save_interval == 0:\n",
    "            accelerator.print('Saving..')\n",
    "\n",
    "            state = {\n",
    "                'net':  bert.state_dict(),\n",
    "                'step': iters,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            accelerator.save(state, log_dir + '/step_' + str(iters) + '.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "177\n",
      "Fine-tuning from checkpoint: /workspace/src/PL-BERT-ID/step_1000000.t7, starting from step 0\n",
      "Start training from step 0...\n",
      "Step [10/1000000], Loss: 12.77700, Vocab Loss: 4.22157, Token Loss: 3.88258\n",
      "Step [20/1000000], Loss: 7.19293, Vocab Loss: 3.06421, Token Loss: 3.32325\n",
      "Step [30/1000000], Loss: 5.79144, Vocab Loss: 2.47776, Token Loss: 2.95963\n",
      "Step [40/1000000], Loss: 5.31915, Vocab Loss: 2.19646, Token Loss: 2.88984\n",
      "Step [50/1000000], Loss: 4.97421, Vocab Loss: 1.66979, Token Loss: 2.86207\n",
      "Step [60/1000000], Loss: 4.59466, Vocab Loss: 1.87071, Token Loss: 2.66714\n",
      "Step [70/1000000], Loss: 4.40943, Vocab Loss: 1.85680, Token Loss: 2.60403\n",
      "Step [80/1000000], Loss: 4.07151, Vocab Loss: 1.15140, Token Loss: 2.46103\n",
      "Step [90/1000000], Loss: 4.03952, Vocab Loss: 1.52484, Token Loss: 2.49397\n",
      "Step [100/1000000], Loss: 3.89068, Vocab Loss: 1.59718, Token Loss: 2.48496\n",
      "Step [110/1000000], Loss: 3.67074, Vocab Loss: 1.23096, Token Loss: 2.31650\n",
      "Step [120/1000000], Loss: 3.54020, Vocab Loss: 1.38603, Token Loss: 2.30231\n",
      "Step [130/1000000], Loss: 3.49574, Vocab Loss: 1.37338, Token Loss: 2.28802\n",
      "Step [140/1000000], Loss: 3.54146, Vocab Loss: 1.15672, Token Loss: 2.15512\n",
      "Step [150/1000000], Loss: 3.34155, Vocab Loss: 1.10932, Token Loss: 2.14814\n",
      "Step [160/1000000], Loss: 3.22641, Vocab Loss: 0.91411, Token Loss: 2.05777\n",
      "Step [170/1000000], Loss: 3.19603, Vocab Loss: 1.01584, Token Loss: 2.09669\n",
      "Step [180/1000000], Loss: 3.21102, Vocab Loss: 1.34232, Token Loss: 2.02673\n",
      "Step [190/1000000], Loss: 3.13482, Vocab Loss: 0.99009, Token Loss: 1.88001\n",
      "Step [200/1000000], Loss: 3.15167, Vocab Loss: 0.89551, Token Loss: 2.07602\n",
      "Step [210/1000000], Loss: 2.98645, Vocab Loss: 0.95039, Token Loss: 2.00742\n",
      "Step [220/1000000], Loss: 2.97832, Vocab Loss: 0.94942, Token Loss: 1.89758\n",
      "Step [230/1000000], Loss: 2.93340, Vocab Loss: 0.85424, Token Loss: 1.92116\n",
      "Step [240/1000000], Loss: 2.95599, Vocab Loss: 0.93121, Token Loss: 2.04908\n",
      "Step [250/1000000], Loss: 2.92742, Vocab Loss: 0.89336, Token Loss: 1.86687\n",
      "Step [260/1000000], Loss: 2.88126, Vocab Loss: 0.92397, Token Loss: 1.92403\n",
      "Step [270/1000000], Loss: 2.88859, Vocab Loss: 1.01498, Token Loss: 1.94466\n",
      "Step [280/1000000], Loss: 2.73925, Vocab Loss: 0.76240, Token Loss: 1.98655\n",
      "Step [290/1000000], Loss: 2.81336, Vocab Loss: 0.88753, Token Loss: 1.89603\n",
      "Step [300/1000000], Loss: 2.74013, Vocab Loss: 0.78962, Token Loss: 1.80862\n",
      "Step [310/1000000], Loss: 2.67659, Vocab Loss: 0.90625, Token Loss: 1.79332\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "while True:\n",
    "    notebook_launcher(train, args=(), num_processes=1, use_port=33389)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PL-BERT-ID",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
